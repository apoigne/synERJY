\chapter{The Reactive Core}\label{core}


\section{Running a First Example.} 

\paragraph{The ``Hello world'' of reactive programming.} The following example can be found in the directory \pp{doc/examples} of the \se\ distribution.
% 
\begin{example}[Basic]\label{Basic}\ 
\codeinput{basic}
\end{example}

Each \se\ program consists of a list of classes. A class is called \emph{reactive} if its constructor ends with an \pp{active} statement. The \pp{active} statement specifies the elementary execution step of an reactive machine, an instant. There may be several reactive classes the interaction of which is explained in Section \ref{reuse}. For the time being we will consider the simple case of just one reactive class.

%
%There should be at least class with a  public static method \pp{main}. We refer to such a class as
%\emph{configuration class}. Here we have only one configuration class \pp{Basic}. 

%A configuration class is the seed from which an application is generated. The procedure \pp{main} is called at system startup and is designed to run forever as long as the method \pp{instant} of the reactive machine returns without indication of an error (\pp{!= 0}).  The method \pp{instant} is generated by the compiler.  It executes an elementary execution step of the reactive machine, an instant.

The field \pp{timing}\index{timing@\pp{timing}} defines the frequency at
which an instant is to be executed. It defines the minimal amount of time
required to execute an instant properly by a real system for worst
case behaviour. Its value is used by the simulator for emulating the real system. In case that execution of any instant of the real system never exceeds the time specified by the timing clause, it will be guaranteed that the simulator exactly reflects the behaviour of the real system. The type \pp{time} is built-in.

To interface to the simulator of the \se\ programming environment, 
there are two particular built-in classes \pp{SimInput}\index{SimInput@\pp{SimInput}}
 and 
\pp{SimOutput}\index{SimOutput@\pp{SimOutput}}
 that implement the interfaces \pp{Input} resp. 
\pp{Output}. These automatically provide proper methods \pp{new\_val} 
and \pp{get\_val}, respectively \pp{put\_val} for interacting with 
the simulator.

% In fact, when running to the simulator, all instances 
%of an implementation of \pp{Input} and \pp{Output} are internally 
%replaced by in instance of \pp{SimInput} or \pp{SimOutput}.

The keyword \pp{active} indicates the synchronous reactive code that is to be executed at every instant. We have a loop. In the loop, a signal \pp{red\_led} is emitted if the (sensor) signal \pp{button} is present, and then control waits for the \pp{next} instant. The reactive code is executed whenever the method instant is called.

All of this will be discussed in detail below.



\paragraph{Running a \se\ program.} We assume that the \se\ programming environment has been installed (cf. \cite{userguide}). After launching \se\ use the the menu \pp{File -> Load project} to open a project. Projects are
directories with specific file .seprj that is comprised of all the necessary information. Open the ``project'' \pp{target/examples/Basic} in the \se\ directory. Loading the project should have compiled the file \pp{basic.se}.  Pressing the button \framebox{{\tt\small Build}} then generates the
simulation code, and pressing the \framebox{{\tt\small Simulate}}
button opens the simulator.

Running the simulator, the sensor \pp{sensor} can be set to
present by clicking on it with the left button in the simulator 
panel. If the sensor \pp{sensor} is set to be present at the first instant,
the signal \pp{actuator} will be emitted, and then nothing will happen
any more. The presence of signals is indicated by a \pp{\$} in the
trace browser.

All this should work out of the box, otherwise please send a note to one of the authors. For further hints of how to handle the \se\ programming environment the user guide should be consulted \cite{userguide}.

\section{Reactive Classes, Sensors, and Signals}\label{classes}

\paragraph{Reactive classes.} \se\ is comprised of a subset of \java\
extended by reactive classes.  
\begin{itemize}
    \item A class is \emph{reactive}\index{class!reactive}
 if it has only one constructor, and if 
    that constructor has a tail of the form
	   \begin{center}
				\pp{active \{ \ldots\ \}},
	    \end{center}
    Such an constructor is called an {\em reactive constructor}.  
\end{itemize}

The operator \pp{active}\index{active@\pp{active}} embeds the synchronous reactive code.  This code is executed at every instant.

\paragraph{Sensors and signals.} Reactive objects communicate by
\emph{sensors}\index{sensor} and \emph{signals}\index{signal}. 
Sensors are read only whereas signals may be updated by the
program.  Sensors may be updated by the environment.

Both sensors and signals may be \emph{present}\index{present} or
\emph{absent}\index{absent}.  A sensor or a signal is present at an
instant if and only if it updated at the same instant.  Otherwise it is
absent.  In the example above, there is one sensor \pp{sensor} and one
signal \pp{actuator}.  The reactive statement \pp{if (?sensor) \{ emit
actuator; \};} checks for the presence of the signal \pp{sensor}.  If
\pp{sensor} is present the signal \pp{actuator} is emitted.

The type \pp{Sensor} indicates that \pp{sensor} is
a sensor while type \pp{Signal} indicates that \pp{actuator} 
is a signal. The objects of type \pp{SimInput}\index{SimInput@\pp{SimInput}} and \pp{SimOutput}\index{SimOutput@\pp{SimOutput}} interface the sensor resp. signal with the simulator. 
They will be discussed in Section~\ref{interface}.

The sensor \pp{sensor} and the signal \pp{actuator} are particular 
in that they do not carry for a value.  We speak of \emph{pure} sensors\index{signal!pure} and \emph{pure} signals in contrast to \emph{valued}\index{signal!valued} sensors and signals
 as in the fragment
% 
\codefragment{valued-signals}
% 
Again we have a sensor and a signal. If the sensor \pp{sensor} is present, the signal \pp{actuator} is emitted with a new value that is 
obtained by increasing the value \pp{\$sensor} of the sensor 
\pp{actuator}. Sensors and signal have a default value according to type, e.g. \pp{0} for \pp{int}.

\paragraph{Sensor and signal types.}
Sensor and signal types are a new kind of built-in reference types. Sensors\index{type!Sensor@\pp{Sensor}}\index{Sensor@\texttt{Sensor}}
 have a type of the form
\begin{center}
\pp{Sensor<$T$>}
\end{center}
where $T$ is a primitive or class type. Operators related to
a sensors are
\begin{quote}
\begin{description}
	  \item[\pp{?$s$} :]\index{\pp{?}} checks whether the sensor $s$ is \pp{present}
	   or \emph{absent}. 
	  The result is of type Boolean.

	  \item[\pp{\$$s$} :]\index{\pp{\$}} yields the \emph{value} of the sensor $s$. 
	  The result is of type $T$.	
\end{description}
\end{quote}
Signals\index{type!Signal@\pp{Signal}}\index{Signal@\texttt{Signal}} have a type of the form
   $$\pp{Signal<$T$>}.$$\index{Signal@\pp{Signal}}
\pp{Signal<$T$>} is a subtype of \pp{Sensor<$T$>} in that the 
\pp{Sensor} operators are inherited. Signals are updated
using the statement
\begin{quote}
  \begin{description}
	  \item[\pp{emit $s(v)$} :] The signal $s$ is emitted to be present
	  at an instant, and the value of $s$ is updated to be $v$.
  \end{description}
\end{quote}
Sensors and signals have a default value\index{default!value}
 depending on the type, e.g.
\pp{0} in case of \pp{int}. This default applies if a sensor or signal has 
not been present yet.

Pure sensors and signals will be considered as being valued of null type.
They have the only value \pp{null}\index{null@\texttt{null}}. We use 
$$\pp{Sensor}\quad resp.\quad \pp{Signal}$$ 
as types for pure sensors and signals.
%\footnote{since the null type cannot be 
%named. However, internally {\tt\scriptsize null} is assumed to be of 
%a type {\tt\scriptsize Null}.}
The emit statement then is of the form \pp{emit $s$}\index{emit@\texttt{emit}}.

\section{Reactive Control}\label{reactive-control}\index{reactive control}


\paragraph{Processes for semantics.} The semantics of reactive
statements is introduced inductively.  For every reactive statement
\texttt{P}, we define a semantic interpretation $P$.  We refer to this
entity as a \emph{synchronous process}\index{process!synchronous}
 or \emph{process}\index{process|textbf}
 for
short.\footnote{We are aware that the term `process' is heavily
overloaded but use it for lack of a better alternative.}
We may understand a synchronous process naively as some piece of executable code that behaves as required by the synchrony hypothesis.  Properties of
synchronous processes relevant for the subsequent discussion are that
\begin{itemize}
	\item  a process may be \emph{active}\index{process!active}
 or \textit{inactive}\index{process!inactive},

	\item state is changed and output is generated only if a process is
	active.

	\item a process may be (\emph{re}) \emph{started}\index{process!start} to turn active,
	and may \textit{terminate}\index{process!terminate} to turn inactive. 
\end{itemize}
A process is called \emph{instantaneous}\index{process!instantaneous}\index{instantaneous} if it terminates at the same
instant it is started, otherwise it is active for more than one
instant.

Since every statement will define a unique process we rather
ambiguously speak of statements as processes mixing syntactic and
semantic level, i.e. a syntactic statement such as \pp{emit $s$}
ambivalently denotes the corresponding process.  This overloading
avoids notational contortions such as $P_{{\tt emit} s}$ to
distinguish syntax and semantics.  This applies as well to an operator
such as \pp{loop \{ $P$ \}}; it denote the
syntactical construction (if $P$ is considered as a piece of code) 
as well as the corresponding semantical operator on processes
(if $P$ is considered as a process).

Processes are defined inductively according to the syntactic 
structure of statements.


\paragraph{Elementary reactive statements.}
The language \se\ has two elementary reactive statements/processes:
when started, the process\index{emit@\pp{emit}}

% 
\BEP
                  emit $s$; 
\EEP
% 
(alternatively \pp{$s$.emit()}) emits the signal $s$ and terminates at
the same instant.  If $s$ is a valued signal, the statement is
% 
\BEP
                  emit $s$($v$);
\EEP
% 
(alternatively \pp{$s$.emit($v$)}) where $v$ is a value of appropriate
type.

% 
The other process\index{next@\pp{next}}
 is
% 
\BEP
                  next;
\EEP
% 
Once started the process terminates only in the next instant.  
These processes are distinguished in that
\begin{itemize}
	\item emitting is \emph{instantaneous}\index{instantaneous}, i.e. the process
	terminates at the same instant it is started.

	\item  the process \pp{next} consumes time, i.e. it starts at 
	one instant but keeps control till the next instant.
\end{itemize}

\paragraph{Sequential composition.}\index{sequential composition}

Composite reactive statements are the sequential and parallel 
composition,
the conditional and the loop statement.  The statements are familiar 
but the semantics may be less.

The \textbf{\emph{sequential composition}} is given by a sequence of 
statements\footnote{Note that ``;'' is a terminator of statements as 
in C, i.e. ``;'' is part of $P_{1}$ and $P_{2}$.}
% 
\BEP
                  $P_{1}$ $P_{2}$
\EEP
% 
Once the composite process is started, the subprocess $P_{1}$ is
started.  When $P_{1}$ terminates, the subprocess $P_{2}$ is started 
in
the same instant.  Hence, for
% 
\codefragment{emit-a-b}
% 
both signals, \pp{a} and \pp{b} are emitted at the same instant. This 
is very different to
% 
\codefragment{emit-a-next-b}
% 
Once this process is started, the subprocess \pp{emit a} is started,
\pp{a} is emitted, the subprocess terminates instantaneously, and the
subprocess \pp{next} is started.  The latter keeps control till the
next instant, i.e. it terminates at the beginning of the next 
instant. 
Only then the subprocess \pp{emit b} is started.  Hence \pp{a} is
emitted in the first instant and \pp{b} in the second
instant.
%\footnote{The presentation is typical for the presentation of
%semantics on this level and for the inductive use of processes.  The
%operators of the language are ambiguously defined for reactive
%statements on syntax level as for processes on semantical level.  The
%semantics of a composite process is defined in terms of the
%subprocesses determined by the sub-statements.}

\paragraph{Traces.}\index{trace|textbf}

It is often convenient to use traces to illustrate the reactive behaviour. \emph{Traces} are sequences of values variables or signals have at an instant. For instance,
% 
\codefragment{emit-x-y-value}
%
has the following traces
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccc}
    \hline\hline   
     \hbox{{\footnotesize \textit{i}}} &{\footnotesize \textit{0}}
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}&\ldots
   \\  
    \hbox{$x$} &$\mathbf{1}$&$\mathit{1}$&$\mathit{1}$&$\mathit{1}$&
    $\mathit{1}$&\ldots
   \\
    \hbox{$y$} &$\mathbf{2}$&$\mathit{2}$&$\mathit{2}$&$\mathit{2}$&
    $\mathit{2}$&\ldots
   \\   \hline\hline
  \end{tabular}
\end{center}
Both \pp{x} and \pp{y} are present at the first instant, and then never 
again (the index $i$ specifies the instants). We use a bold typeface to
 indicate that a signal is present, and italic to indicate that the value can be read only. To give another example, consider
% 
\codefragment{emit-x-next-y-value}
%
The behaviour is specified by the 
trace diagram 
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccc}
    \hline\hline   
     \hbox{{\footnotesize \textit{i}}} &{\footnotesize \textit{0}}
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}&\ldots
   \\  
   \hbox{$x$} &$\mathbf{1}$&$\mathit{1}$&$\mathit{1}$&
   $\mathit{1}$&$\mathit{1}$&\ldots
   \\
    \hbox{$y$} &$\mathit{0}$&$\mathbf{2}$&$\mathit{2}$&
    $\mathit{2}$&$\mathit{2}$&\ldots
   \\   \hline\hline
  \end{tabular}
\end{center}
The signal \pp{x} is present in the first instant, and its value set to $1$.
The signal \pp{y} is present only in the second instant with its value being set to $2$ In the first instant it has a default value ($0$ for integers).

For pure signals, only presence\footnote{and not the value that is always \pp{null}.} is relevant. Hence we modify notation and indicate presence by
an  asterisk \pp{*} and absence by a dot, as in 
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccc}
    \hline\hline   
     \hbox{{\footnotesize \textit{i}}} &{\footnotesize \textit{0}}
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}&\ldots
   \\  
    \hbox{$a$} &$*$&.&.&.&.&\ldots
   \\
    \hbox{$b$} &.&$*$&.&.&.&\ldots
   \\   \hline\hline
  \end{tabular}
\end{center}
This trace diagram reflects the behaviour of
% 
\codefragment{emit-a-next-b}
%

\paragraph{Conditional.} The \textbf{\emph{conditional}}\index{conditional}
 has the format
% 
\BEP
               if ($C$) \{ $P_{1}$ \} else \{ $P_{2}$ \};
\EEP
% 
It behaves as to be expected. Once started, it evaluates the condition $C$ and then, at the same instant, starts -- according to the result -- either $P_{1}$ or $P_{2}$. It terminates whenever either $P_{1}$ or $P_{2}$ terminate. The expression $C$ must be Boolean.

Note that a conditional may terminate at different instants, depending on
which branch has been chosen. Consider
%
\codefragment{conditional}
%
In case that the signal \pp{a} is present, the conditional terminates at the same instant when started. Hence the signals \pp{b} and \pp{d} are emitted when \pp{a} is present.
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccc}
    \hline\hline   
     \hbox{{\footnotesize \textit{i}}} &{\footnotesize \textit{0}}
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}&\ldots
   \\  
    \hbox{$a$} &$*$&.&.&.&.&\ldots
   \\
    \hbox{$b$} &$*$&.&.&.&.&\ldots
   \\
    \hbox{$c$} &.&.&.&.&.&\ldots
   \\
    \hbox{$d$} &$*$&.&.&.&.&\ldots
  \\   \hline\hline
  \end{tabular}
\end{center} 
In contrast, if \pp{a} is absent the signal \pp{c} is emitted at the same instant, but \pp{d} only at the next instant.
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccc}
    \hline\hline   
     \hbox{{\footnotesize \textit{i}}} &{\footnotesize \textit{0}}
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}&\ldots
   \\  
    \hbox{$a$} &.&.&.&.&.&\ldots
   \\
    \hbox{$b$} &.&.&.&.&.&\ldots
   \\
    \hbox{$c$} &.&$*$&.&.&.&\ldots
   \\
    \hbox{$d$} &.&.&$*$&.&.&\ldots
  \\   \hline\hline
  \end{tabular}
\end{center} 

\paragraph{The loop.}\index{loop@\pp{loop}}

Finally, when started, the \textbf{\emph{loop}}
% 
\BEP
                     loop \{ $P$ \};
\EEP
% 
starts the body $P$.  When $P$ terminates, $P$ is restarted \emph{at the
same instant}.  A loop runs forever.  It is required that the body $P$
is \emph{not} instantaneous, i.e. the body should not terminate in 
the same instant it starts.  Otherwise, a infinite sequence of 
computations must be executed at an instant, which is not acceptable, of course. 

A small example
%
\codefragment{loop-emit}
%
may illustrate the behaviour:
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccc}
    \hline\hline   
     \hbox{{\footnotesize \textit{i}}} &{\footnotesize \textit{0}}
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}&\ldots
   \\  
    \hbox{$a$} &.&$*$&$*$&$*$&$*$&\ldots
   \\
    \hbox{$b$} &.&$*$&$*$&$*$&$*$&\ldots
   \\
    \hbox{$c$} &$*$&.&.&.&.&\ldots
    \\   
    \hline\hline
  \end{tabular}
\end{center}
In the first instant, \pp{a} is absent, and \pp{c} is emitted since
the condition evaluates to false. At the next instant,
the signal \pp{a} is emitted, the body of the loop terminates, and the loop
is restarted. Since \pp{a} is present now, \pp{b} is emitted. This behaviour
persists forever.

\paragraph{Parallel composition.}When the \textbf{\emph{parallel composition}}\index{parallel composition}
 
% 
\BEP
                   [[ $P_{1}$ || $P_{2}$ ]];
\EEP
% 
is started, both $P_{1}$ and $P_{2}$ are started.  The parallel
composition terminates either when both $P_{1}$ and $P_{2}$ terminate 
in the same instant or when one process terminates and the other process
has terminated earlier.

Here is an example where both the subprocesses terminate at the same instant
%
\codefragment{parallel}
%
The corresponding trace is
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccc}
    \hline\hline   
     \hbox{{\footnotesize \textit{i}}} &{\footnotesize \textit{0}}
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}&\ldots
   \\  
    \hbox{$a$} &.&$*$&.&.&.&\ldots
   \\
    \hbox{$b$} &.&$*$&.&.&.&\ldots
   \\
    \hbox{$c$} &.&$*$&.&.&.&\ldots
    \\   
    \hline\hline
  \end{tabular}
\end{center}
Both the subprocesses terminate at the same instant namely when
the signals \pp{a} and \pp{b} are emitted.

The subprocesses in
%
\codefragment{parallel-next}
%
terminate at different instants: \pp{emit a} instantaneously, whereas
the other terminates at the next instant:
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccc}
    \hline\hline   
     \hbox{{\footnotesize \textit{i}}} &{\footnotesize \textit{0}}
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}&\ldots
   \\  
    \hbox{$a$} &$*$&.&.&.&.&\ldots
   \\
    \hbox{$b$} &.&$*$&.&.&.&\ldots
   \\
    \hbox{$c$} &.&$*$&.&.&.&\ldots
    \\   
    \hline\hline
  \end{tabular}
\end{center}

A parallel process never terminates if one of its subprocesses never terminates.
%
\codefragment{parallel-loop}
%
The corresponding trace is
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccc}
    \hline\hline   
     \hbox{{\footnotesize \textit{i}}} &{\footnotesize \textit{0}}
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}&\ldots
   \\  
    \hbox{$a$} &$*$&.&.&.&.&\ldots
   \\
    \hbox{$b$} &$*$&$*$&$*$&$*$&$*$&\ldots
   \\
    \hbox{$c$} &.&.&.&.&.&\ldots
    \\   
    \hline\hline
  \end{tabular}
\end{center}


\paragraph{Wavefront computation\index{wavefront computation}
 and causality\index{causality}.}\label{causality} The 
synchrony hypothesis demands that every signal has a unique status at 
an instant.  This has some impact on the scheduling of computations.  
Let us consider a simple example
% 
\codefragment{wave-front} 
% 
We assume that, when starting the process, the status of each signal
is unknown.  Because of the top-level parallel composition all the
three subprocesses are started.  Each of the conditionals
instantaneously checks its condition.  Since the status of the signals
is not known the conditionals are frozen till sufficient information
is available to evaluate the condition.  Process (2) emits the signal
\pp{a} and terminates.  The evaluation of the frozen conditionals is
resumed since more information about the status of signals has been
accumulated.  The conditions (1) and (3) now evaluate to true, and the
signals \pp{b} and \pp{c} are emitted.  The conditionals terminate, 
hence the parallel composition as well.

This evaluation strategy is a consequence of the coherence condition (cf. Section~\ref{synchronous-programming}).\index{coherence}
 
A (sub-) process is suspended if required information about the status
of signals is not available, and it resumes computation if the
information is available.  In that more information is gradually
accumulated.  If there are only suspended processes, we can assume that
a signal, the status of which is still unknown, can only be absent,
since, by coherence, all sensors or signals that are not emitted yet or not
present as input must be absent.  With this additional information the
wavefront computation is resumed.  For instance, let us assume that, 
when starting
% 
\codefragment{coherence} 
% 
the status of \pp{a} is still unknown.  The both the conditionals are
suspended.  Now we conclude by coherence that \pp{a} must be absent. 
Using this the conditionals evaluate to false, and the parallel
process terminates instantaneously.  Roughly, this is the
\emph{constructive semantics}\index{constructive semantics} of Berry~\cite{constructivesemantics}.

This evaluation strategy has a -- maybe -- unforeseen consequence.  Consider
the statement
% 
\codefragment{causal-if} 
% 
If the status of \pp{a} is unknown, the process is suspended till we can
assume that \pp{a} is absent.  The condition is evaluated and \pp{a} 
is
emitted.  Hence \pp{a} becomes present -- in contradiction to the
assumption we made earlier.  We violate the coherence condition in
that the signal \pp{a} is considered present and absent at the same
instant.  We speak of a \emph{causality cycle}\index{causality!cycle}
since presence of a signal depends on its own status.

For good reasons we use a more restrictive
interpretation of causality: 
\begin{itemize}
\item Once the status of a signal has been used, 
the status cannot be changed any more in the same instant.
\end{itemize}
According to this strategy
% 
\codefragment{emit-a-causal-if} 
% 
is rejected since, after checking the presence of \pp{a} in the 
condition, \pp{a} may be emitted. In contrast, the constructive 
semantics would see that \pp{a} is present before evaluating the 
conditional, hence would emit \pp{a}.

Our strategy, we tag as ``write-before-read''\index{write-before-read}, 
strictly depends on the 
syntactical structure. Any causality cycle in any sub-statement will 
raise a causality error.\footnote{Whether this strategy is too 
restrictive or not is a matter of discussion. It has the merit of 
being reasonably transparent while the constructive semantics may 
accept programs for which it is sometimes hard to grasp why there no 
causality cycle
%(see Appendix~\ref{semantics}, Section~\ref{causality})
.}

Valued signals are another source for causality cycles as in
%
\codefragment{causal-data} 
% 
the value of \pp{a} is read before it is written (emitted). Note that
% 
\codefragment{causal-data-direct} 
% 
leads to a causality cycle as well; though the value of \pp{a} is 
consistently the same, we have to read the value before we write it.

The analysis of causality is computationally expensive.  The control
and signal flow as well as the user specified precedence relations
generate a partial order.  All the reactive
statements are \emph{scheduled at compile time}\index{scheduling} according to this
partial order relation thus guaranteeing deterministic evaluation of
all programs accepted by the compiler.
% \footnote{In future, we envisage to use a more sophisticated
% analysis based on an in-depth data flow analysis and symbolic
% presentation techniques.}. 
The compiler will reject programs that have a causal cycle raising an
error message that indicates which parts of the program are involved
in the cycle.

\paragraph{Weak and strong preemption.}\index{preemption|textbf} Apart from 
parallel composition and the infinitary loop synchronous programs are 
pretty conventional so far. The real power of synchronous programming 
stems from the preemption operators. Preemption means that execution 
of some process  --  an infinitary loop for instance -- may be 
cancelled under some condition to start some other process 
instantaneously. We say that a process is \emph{preempted}.  There 
are two operators for preemption of a process.

The operator for \textit{weak preemption}\index{preemption!weak}
%
\BEP
cancel $P$ when ($C$);
\EEP
%
cancels the process $P$ if the condition $C$ becomes true, but only 
for the \emph{next} instant, that is: evaluation of the condition $C$ 
is scheduled after that of the process $P$.  Hence $P$ may influence 
the result of evaluating $C$.

The operator for \textit{strong preemption}\index{preemption!strong}
%
\BEP
cancel strongly $P$ when ($C$);
\EEP
%
cancels the process $P$ at the \emph{same} instant in which the
condition $C$ becomes true, that is: the evaluation of the condition 
$C$ is scheduled before that of the process $P$.  Hence $P$ cannot 
influence the result of evaluating $C$.

A trivial example demonstrates the difference. If \pp{b} is present in
%
\codefragment{cancel} 
%
the cancel statement terminates, but \pp{a} is emitted before:  
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccc}
    \hline\hline   
    \hbox{$a$} &$*$&.&.&.&.&\ldots
   \\
    \hbox{$b$} &$*$&.&.&.&.&\ldots
   \\
    \hbox{$c$} &$*$&.&.&.&.&\ldots
    \\   
    \hline\hline
  \end{tabular}
\end{center}

In contrast, \pp{a} is not emitted any more in
%
\codefragment{cancel-strongly} 
%
when \pp{b} is present:
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccc}
    \hline\hline   
    \hbox{$a$} &.&.&.&.&.&\ldots
   \\
    \hbox{$b$} &$*$&.&.&.&.&\ldots
   \\
    \hbox{$c$} &$*$&.&.&.&.&\ldots
    \\   
    \hline\hline
  \end{tabular}
\end{center} 

Weak preemption allows to preempt, for instance, a loop from the 
inside:
%
\codefragment{cancel-loop} 
%
Since the \pp{emit a} is executed before \pp{?a} the loop is 
preempted instantaneously if started.  Strong preemption causes a
causality cycle here; since evaluation of the condition \pp{?a} is
scheduled before that of the loop there is a potential write before 
read of the signal \pp{a}.

The general scheme for preemption operators is this:
%
\BEP
cancel [ strongly ]
    $P$
when ($C_{1}$) [ \{ $P_{1}$ \} ]
[ else when ($C_{2}$) [ \{ $P_{2}$ \} ]
  \ldots
  else when ($C_{n}$) [ \{ $P_{n}$ \} ]
];
\EEP
%
The conditions are executed in the obvious order. If the condition 
$C_i$ holds, the process $P_i$ is executed. The clauses in square 
brackets are optional.

\paragraph{The await statement.}\index{await@\pp{await}}
 The statement \pp{await $C$;} is a 
shorthand for
%
\BEP
cancel \{
   loop \{ next; \};
\} when ($C$);
\EEP
%
The process waits, possibly for several instants, till the condition 
$C$ becomes true. This obviously is a useful and often used construct 
-- for instance when waiting for signal to become present (\pp{await 
?a}). Note that the await statement may terminate immediately when 
started.\footnote{The operator \pp{await} in \se\ is equivalent to 
the operator \pp{await immediate}\index{await@\texttt{await}!immediate} of \esterel.} 

The \pp{await} operator is often used when simulating automata-like 
behaviour\index{automaton!imperative style}
 as in
%
\codefragment{state-loop} 
%
This encodes two ``states''; if in the state 1 ,and if \pp{a} is 
present, a ``transition'' from state 1 to state 2 takes place and the 
signal \pp{b} is emitted.   Similarly, a ``transition'' from state 2 
to state 1 takes place if \pp{a} is present, emitting \pp{d}. A possible trace is
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccccccccc}
    \hline\hline   
     \hbox{{\footnotesize \textit{i}}} &{\footnotesize \textit{0}}
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}
     &{\footnotesize \textit{5}}&{\footnotesize \textit{6}}
     &{\footnotesize \textit{7}}&{\footnotesize \textit{8}}
     &{\footnotesize \textit{9}}&\ldots
   \\  
    \hbox{$a$} &.&.&$*$&.&.&$*$&.&.&$*$&.&\ldots
   \\
    \hbox{$b$} &.&.&$*$&.&.&.&.&.&$*$&.&\ldots
   \\
    \hbox{$c$} &.&.&.&.&.&$*$&.&$*$&.&.&\ldots
    \\   
    \hbox{$c$} &.&.&.&.&.&$*$&.&.&.&.&\ldots
    \\   
    \hline\hline
  \end{tabular}
\end{center} 
Note that the presence of \pp{a} does not affect the behaviour when in ``state'' 2, resp. presence of \pp{c} when in ``state'' 1.


\paragraph{Activation.} Sometimes it is convenient to activate a 
process only if some condition holds. Then the \emph{activation 
statement}\index{activate@\pp{activate}}

% 
\BEP
activate $P$ when ($C$);
\EEP
% 
should be used where $P$ being a process, and $C$ being a Boolean
expression.  If started the process waits until the condition $C$
becomes true.  Then $P$ is started and executes at every instant the
condition $C$ is true.  Activation down-samples a process with a
frequency specified by $C$.

In the example
%
\codefragment{activate}
%
the loop process executes only if the signal \pp{b} is present. A possible trace is
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} ccccccccccc}
    \hline\hline   
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}
     &{\footnotesize \textit{5}}&{\footnotesize \textit{6}}
     &{\footnotesize \textit{7}}&{\footnotesize \textit{8}}
     &{\footnotesize \textit{9}}&\ldots
   \\  
    \hbox{$a$} &.&.&$*$&$*$&.&$*$&$*$&$*$&$*$&.&\ldots
   \\
    \hbox{$b$} &.&.&$*$&$*$&.&$*$&$*$&$*$&$*$&.&\ldots
    \\   
    \hline\hline
  \end{tabular}
\end{center} 


\paragraph{Sustaining.}\index{sustain@\texttt{sustain}}
Another convenient operator is sustaining an instantaneous process
\index{process!instantaneous}
% 
\BEP
sustain \{ P \};
\EEP
% 
When started the the sustain statement executes at each instant the 
instantaneous process $P$ (that is, $P$ terminates at the instant it 
is started. The statement never terminates. The statement is 
an abbreviation of
% 
\BEP
loop \{ 
   P 
   next;
\};
\EEP
% 
but is often handy, for instance to emit a signal continually
% 
\BEP
sustain \{ emit s; \};
\EEP
%

\paragraph{\textit{Delaying reactions.}}\index{delay|textbf}

{\em
We often distinguish between what happens at the first instant when a 
process $P$ is started, and what happens at later instants. We use 
$P_\alpha$ to refer to the behaviour of $P$ at the first instant, and 
$P_\beta$ to refer to the behaviour at later instants. Given these 
preliminaries, a variation of the preemption and the activation 
operator can be defined that are useful at times, namely that either 
are not applied at the first, but only at later instants. These 
"delayed operators" indicated by a \emph{\pp{next}} are defined by
\index{activate@\pp{activate}!next@\pp{next}}
\index{cancel@\pp{cancel}!next@\pp{next}}

\begin{center}
\emph{
\begin{tabular}{lll}
  \pp{cancel $P$ next when ($C$);}  & $\cong$ &
  \pp{$P_\alpha$; cancel $P_\beta$ when ($C$);} \\
  \pp{activate $P$ next when ($C$);} & $\cong$ & 
  \pp{$P_\alpha$; activate $P_\beta$ when ($C$);} 
\end{tabular}
}
\end{center}

We consider an example. Let \emph{$P = \pp{loop \{next;\};}$}. The 
\emph{\pp{next}} is executed at the first instant, and the loop is 
iterated at later instant, i.e. \emph{$P_\alpha = \pp{next}$} and 
\emph{$P_\beta = \pp{loop \{next;\};}$}. Thus 
\BEP
cancel loop \{next;\}; next when ($C$);
    $\cong$ next; cancel loop \{next;\}; when ($C$);
\EEP 
Hence the process waits at least for one instant and then terminates 
if the conditions $C$ becomes true. We introduce the shorthand 
\emph{\pp{await next $C$;}} for this process. Note that this process 
is equivalent to \emph{\pp{next;await $C$;}} (though \emph{\pp{await 
next $C$;}} has a more efficient implementation).

The \pp{await next}\index{await@\pp{await}!next@\pp{next}} operator is particularly useful when simulating automata-like behaviour.
%
\codefragment{await-next} 
% 
is a somewhat more intuitive (and efficient) alternative for the 
example considered two paragraphs above.

The general scheme for delayed preemption\index{preemption!delayed}
is\footnote{The construct 
\pp{cancel strongly $P$ next when $C$} is known as {\tt do $P$ 
watching $C$} in \esterel.}
%
\BEP
cancel [strongly]
    $P$
next when $C_{1}$ [ \{ $P_{1}$ \} ]
   [ else when $C_{2}$ \{ $P_{2}$ \}
     \ldots
     else when $C_{n}$ \{ $P_{n}$ \};
   ]
\EEP
%
A small example might highlight the differences
%
\codefragment{delayed-strong-cancel}
%
The signal \pp{a} is emitted when the process is started even if the signal
\pp{b} is present since preemption does not apply in the first instant, as shown in the trace
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} ccccccc}
    \hline\hline   
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}
     &{\footnotesize \textit{5}}&{\footnotesize \textit{6}}&\ldots
   \\  
    \hbox{$a$} &$*$&$*$&$*$&.&.&.&\ldots
   \\
    \hbox{$b$} &$*$&.&.&$*$&.&.&\ldots
   \\
    \hbox{$c$} &.&.&.&$*$&.&.&\ldots
    \\     
    \hline\hline
  \end{tabular}
\end{center} 

}


\newpage
\section{Local Signals and Reincarnation}\label{local-signals}

\paragraph{Local signals.}\index{signal!local}

Local variables, in particular local signals, may be declared within a block, i.e. within a context of the form \texttt{\{ \ldots \}}. The scope of a local variable declaration in a block is the rest of the block in which the declaration appears, starting with its own declaration.
Local signals declarations are final, i.e. must be of the form
\index{new@\pp{new}}
\begin{center}
\texttt{Signal<$T$> $identifier$\ = new Signal<$T$>();}\\
\texttt{Signal $identifier$\ = new Signal();}
\end{center}
Due to this restriction, initialisation\index{signal!initialisation} may be dropped for notational convenience, i.e. \texttt{Signal<$T$> $identifier$} resp. \texttt{Signal $identifier$} is sufficient to
declare a local signal.

\paragraph{Reincarnation.}\index{reincarnation}

Local declarations may result in a behaviour that may be unexpected. Consider
%
\codeinput{reincarnation}
%
If the loop is entered, a new incarnation\index{incarnation!local signal}
$x'$ of the local signal \pp{x}
is created. Next the conditonal is executed. Since \pp{x} has not been emitted, the signal \pp{a} is emitted. In the next instant \pp{x} is emitted, 
the actual incarnation of \pp{x} being $x'$. Hence $x'$ is present,
and \pp{c} is emitted. The loop terminates, and the incarnation $x'$ is
not accessible any more. The loop is restarted, a new incarnation $x''$ of \pp{x} is created, and so on. The trace diagram illustrate this behaviour
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccc}
    \hline\hline
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}&\ldots
   \\  
    \hbox{$a$} &.&.&.&.&\ldots
   \\ 
   \hbox{$b$} &$*$&$*$&$*$&$*$&\ldots
   \\
   \hbox{$c$} &.&$*$&$*$&$*$&\ldots
   \\
   \hbox{$x'$} &.&$*$&&&\ldots
   \\
   \hbox{$x''$} &&$.$&$*$&&\ldots
   \\
   \hbox{$x'''$} &&&.&$*$&\ldots
   \\
   \hbox{$\ldots$} \ldots
   \\   \hline\hline
  \end{tabular}
\end{center}
We note that two incarnations coexist \emph{at the same instant}, one being present, the other not. This is a consequence of synchrony: restarting the 
loop is instantaneous, only the \pp{next} statement ``consumes time''.

We observe further that incarnations of local signals have a restricted 
lifetime. In case of the example, each incarnation is accessible only for two instants: when it is created when (re-) entering a loop cycle, and when 
the respective cycle terminates. In the trace diagram, the lack of an entry indicates that a signal/incarnation is inaccessible. 

\paragraph{Multiple evaluation of statements.}
\index{statement!multiple evaluation of}

Statements may even be evaluated more than once at one instant:
%
\codeinput{gonthier}
%
The effect is thus: the conditional is evaluated three times at the same
instant, depending on the incarnations of the signals \pp{x} and \pp{y}.
Let us look at the second instant. Because cancelling is weak, we have to evaluate what is inside before we check for the cancellation condition. 
Hence \pp{y} is emitted to make incarnation $y'$ present, similarly for 
$x'$ because it is in parallel. Now the conditional is evaluated with 
$x'$ and $y'$ being present. Hence \pp{a} is emitted. 
Now the inner cancel statement is executed, the surrounding loop terminates. Then the second incarnation $y''$ of \pp{y} is created. $y''$ is absent since 
there is no reason why it should be present. The conditional is evaluated for a second time, with $x'$ being present, and $y''$ being absent. Now all the computations within the outer \pp{cancel} statement are executed, the cancellation condition applies, and the outer loop is restarted. The new 
incarnation $x''$ of \pp{x} is created as well as a third incarnation $y'''$ of \pp{y}, and the conditional is evaluated
for a third time, now with both $x''$ and \pp{y'''} being absent. Hence \pp{d} is emitted.
\begin{center}
  \leavevmode
  \begin{tabular}[]{l@{\quad}||@{\quad} cccccc}
    \hline\hline
     &{\footnotesize \textit{1}}&{\footnotesize \textit{2}}
     &{\footnotesize \textit{3}}&{\footnotesize \textit{4}}&\ldots
   \\  
    \hbox{$a$} &$\_$&$*$&$*$&$*$&\ldots
   \\ 
   \hbox{$b$} &$\_$&$*$&$*$&$*$&\ldots
   \\
   \hbox{$c$} &$\_$&$\_$&$\_$&$\_$&\ldots
   \\
   \hbox{$d$} &$*$&$*$&$*$&$*$&\ldots
   \\   
   \hbox{$x'$} &.&$*$&&&\ldots
   \\
   \hbox{$x''$} &&$.$&$*$&&\ldots
   \\
   \hbox{$y'$} &.&$*$&&&\ldots
   \\
   \hbox{$y''$} &&$.$&$*$&&\ldots
   \\
   \hbox{$y'''$} &&$.$&$*$&&\ldots
   \\   \hbox{$\ldots$} &\ldots   
   \\   \hline\hline
  \end{tabular}
\end{center}


\section{\textit{Breaking Causality Cycles.}}

\paragraph{\textit{Delaying emittance.}}\index{emit@\pp{emit}!delayed} 
{\em Causality cycles sometimes are a nuisance when programming 
synchronously.  They tend to raise unexpectedly, and they may be 
difficult to break. Using \emph{delayed signals}\index{signal!delayed}
 is a means to break 
a causality cycle. Delayed signals are emitted at \emph{one} instant 
but are present with a new value at the \emph{next} instant only. 

The signal types
\begin{center}
    \emph{\pp{DelayedSignal}} \quad and \quad 
\emph{\pp{DelayedSignal<$T$>}}
\end{center}
(or \emph{\pp{Delayed}}, \emph{\pp{Delayed<$T$>}} for short) are subtypes of \emph{\pp{Sensor}} resp. \emph{\pp{Sensor<$T$>}}. The semantics of the emit statement changes for delayed signals. 
\begin{quote}

\begin{description}
\item[\pp{emit $s(v)$} :] The signal $s$ is emitted (at one instant) to be 
present at the next instant. The value of the signal is updated only at the 
next instant as well.
\end{description}

\end{quote}

Let is analyse a small example.
% 
\codeinput{delayed-emit}
% 
Since $a$ is emitted for the next instant it is not present in the 
first instant, and $b$ is not emitted. The \emph{\pp{next}} process 
is started to terminate in the next instant. Then \emph{\pp{a}} is 
present and \emph{\pp{c}} is emitted.

A delayed signal cannot occur in a causality cycle since its status 
can be
determined at the beginning of an instant (just like that of an input
signal), having been emitted, if so, at the previous instant. Hence
there is no causality cycle in
%
\codeinput{delayed-no-causality}
%
The conditional process is suspended till the condition can be 
evaluated. Since the signal \emph{\pp{a}} is delayed, it is present 
only if it has been emitted at the previous instant. Let us assume 
that this is not the case, i.e. the signal \emph{\pp{a}} is absent at 
the present instant. Then the signal \emph{\pp{a}} is emitted, but 
only for the next instant. Hence it is still (consistently) absent at 
the present instant, and no causality cycle is generated. If is was 
present at the previous instant, \emph{\pp{b}} is emitted at the 
present instant.

There has been a certain amount of discussion about the benefits of 
delayed signals (in particular with regard to 
\statecharts,\index{Statecharts} cf. 
Section~\ref{visual}). We believe that both signals and delayed 
signals have their merits and their shortcomings. Delayed signals get 
rid of causality cycles which are notoriously difficult to debug. On 
the other hand, a causality cycle possibly brings to light some 
misinterpretation of code already at compile time. In contrast, it is 
often difficult to compute the delays introduced by delayed signals. 
A mistake will become manifest only at run time, hence might be 
difficult to debug as well.
}
%\newpage
%\paragraph{\textit{Accessing the previous value.}}\label{previous}

%{\em If the causality cycle is on data as in
%%
%\BEP
%emit $s$($s$ + 1); 
%\EEP
%%
%there is another method of sensors and signals that comes handy.
%\begin{itemize}
%\item the parameterless method \emph{\pp{previous}} accesses the value a sensor or signal had \emph{before} it was emitted for the last time.
%\end{itemize}

%Because $s.\pp{previous}()$ refers to a value $s$ had at an instant that is not the present one the statement
%%
%\BEP
%emit $s$($s$.previous() + 1);
%\EEP
%%
%does not cause a causality cycle, and captures what one probably had in mind writing the statement \emph{\pp{emit $s$($s$+1);}}. We again have a shorthand for calling the method \emph{\pp{previous}}
%%
%\BEP
%emit $s$(pre($s$) + 1);
%\EEP
%%
%The operator \emph{\pp{pre}} will play a crucial role when discussing data-flow models of synchronous programming in Chapter~\ref{data-flow}
%

%}

%\newpage
\section{Embedding Data}\label{Data}

\paragraph{Calling data methods.} Data methods\index{data method}
may be called either 
as an atomic statement or as an atomic Boolean condition.
%
\codeinput{counter} 
% 
The loop increments a counter.  The loop is preempted if the counter 
is incremented to $5$ because the data method \pp{isElapsed} then 
evaluates to true.

Calls of data method must be \emph{atomic}\index{data method!atomic}
 and \emph{instantaneous}\index{data method!instantaneous}, i.e.
when started, the respective method is evaluated, and the
process must terminate instantaneously.
As a consequence, fields may have several values at an instant. For 
instance, if started
%
\codefragment{causal-decrement-reset}
% 
increments the field \pp{counter} and then resets it at the same 
instant.

The requirement that 
data methods are considered as atomic and instantaneous in a reactive 
context imposes considerable constraints on data methods: 
the execution time of a data method should not depend on the parameter values, being e.g. a 
list or a stack. This would violate the synchrony hypothesis in that 
a worst execution time cannot be predicted at compile time.

Assignments may as well be used as elementary statements in reactive
code. Using assignments the example above may be rewritten to (the
somewhat simpler)
%
\codeinput{counting-assign} 
%

\paragraph{Time races.} Calling data methods may result in a time
race\index{time race}, i.e. the execution order of two or more calls of data
methods at the same instant cannot be determined from the program structure. Consider the methods \pp{increment} and \pp{isElapsed} as defined above in
%
\codefragment{causal-decrement-isElapsed}
% 
In this fragment, the counter may first be incremented and 
then asked whether it has elapsed, or the other way round. 
The results will clearly be different. Since no order of execution is prescribed, evaluation may take place in any order resulting in non-deterministic behaviour. 

The \se\ compiler rejects such programs with an error message. Such time races, of course, occur as well with regard to assignments
%
\codefragment{causal-decrement-isElapsed-assign}
%  

Let us speak of a \emph{potential time race}\index{time race!potential}
 if several data methods or
assignments are called at the same instant. Often the program structure
provides enough information to resolve potential time races. For instance, in the fragment
%
\codefragment{counter-fragment}
%
the methods \pp{increment} and \pp{isElapsed} may be called at the same instant. However, the call of the method \pp{increment} is always executed
 before that of \pp{isElapsed} according to the definition of the weak 
 cancel operator: the body of the cancel statement must be evaluated before
 the the condition.

As a general rule, time races occur between statements that reside in different branches of a parallel statement as above. However, the signal flow may restrict the order of computation so that even such potential time
races are resolved as in, e.g.
%
\codefragment{potential-timerace}
%    
According to the wavefront computation\index{wavefront computation}
emittance takes place prior to evaluation of the conditional,
 hence the method \pp{increment} is called
before the method \pp{decrement}.

As stated earlier, the compiler issues an error message if it cannot
resolve a potential time race. In that case the user may add a \emph{precedence statement}\index{precedence} such as, e.g.,
    %
    \codefragment{precedence-increment-isElapsed}
    %
It specifies that, whenever involved in potential time races,
\begin{itemize}
\item the parameterless method \pp{increment} is always executed before the method \pp{isElapsed},

\item the assignment to the field \texttt{counter} precedes the access to
the field \texttt{counter} and that

\item the method \pp{increment}  is always executed before the method \pp{add} with two parameters of type \pp{int}, 
\end{itemize}

Note that the precedence is general: if involved in
a potential time race, \emph{any} call of the method \pp{increment} is
executed before any call of the method \textit{isElapsed}. Similarly,
any assignment to the field
\texttt{counter} must be executed before \emph{any} access of
    the field \texttt{counter}.

The precedence statement introduces a new kind of precedence relations 
on data methods, assignments, and field accesses. The precedence
relations are specified using the following patterns.
\begin{itemize}
   \item The \emph{data method pattern} is determined by
   $m(T_{1},\ldots,T_{n})$, i.e. a method name $m$ and the number and types 
   $T_{1}$,\ldots,$T_{n}$ of arguments (i.e. the signature of the method).
   
   \item The \emph{assignment pattern} is determined by 
   $(f=)$ with $f$ being a field name. The notation \texttt{($f$=)} is
    meant to abstract the various assignment and increment operators that
    change the field $f$, 
   e.g. \texttt{=}, \texttt{++}, \texttt{+=}, etc.. 
   
   \item The \emph{field access pattern} is determined by the name $f$ of a
   respective field.
\end{itemize}
Let $a$ and $b$ be two such patterns. The precedence may be specified 
using two kinds of relations\index{precedence!specification}: 
\begin{quote}
    \begin{enumerate}
	\item[\texttt{a < b} :] all occurrences of the pattern \pp{a}
	must be scheduled before any occurrence of the pattern \pp{b}.
                    
	\item[\texttt{a*} :] calls of \pp{a} may be scheduled in
	any order.

    \end{enumerate}
\end{quote}
The precedence relations may be combined in a precedence clause. 
Combinations are, e.g. 
\begin{quote}
  \begin{description}

	\item[\texttt{a* < b} :]\ \\
	 all calls of \pp{a} must be scheduled
	before \pp{b} and all calls of \pp{a} may be scheduled in any
	order.
    
  \end{description}
\end{quote}

Note that control flow and precedence statements may contradict each
other as in the code fragment
%
\BEP
\ldots

if (counter > 0) \{ counter-- \};

\ldots

precedence \{
    (counter =) < counter;
\};
\EEP
% 
According to the control flow the field \texttt{counter} must be accessed before the value of field may eventually be decreased by one. On the other
hand the precedence close states that access to the field should take place
only after all updates of the field have been executed. Obviously, we cannot
satisfy both these scheduling requirements. Hence a causality error must be
raised.

For a general strategy, the program structure will determine the 
causal dependencies most of the times. Only a parallel statement may 
cause a time race. In that case, precedence clauses should be added to 
eliminate non-determinism. A second choice, may be to add precedence 
clauses for semantic control; for instance, if some data actions 
should always be scheduled after some others, it may be a good idea 
to 
state this explicitly to exclude that an unwanted scheduling order is induced 
from the program structure. Be aware however, that, the more precedence 
clauses are introduced, the likelihood of a causality cycle 
increases. 
Hence precedence statements should be used with care. 


\paragraph{Multiple emits for valued signals.}\label{timeraces}
\index{multiple emit}\index{time race!multiple emit}
Another type of ``time race'' occurs with regard to valued signals.
A signal may be emitted several times at the same instant as in
%
\codefragment{time-race-valued-emits}
%
Depending on the order of evaluation, the value of the valued signal
\pp{x} may be $1$ or $2$.  The compiler again rejects such a program
because of non-determinism except for one exception:
\begin{itemize}
	\item  if a signal is pure (i.e. of type \pp{Signal}).
\end{itemize}
In that case no conflict can arise since due to the lack of values.

Note, however, that the signal \pp{x} will have the value $2$ after 
evaluating
%
\codefragment{causal-valued-emits}
%
The sequential composition imposes a causal order. 

\paragraph{\textit{Labels for sequencing.}}\index{label|textbf}
{\em Labeling is a new concept for synchronous languages that supports
a finer control of precedence that precedence statements do. Labels 
refer to individual statements within a reactive program.  The notation 
is\index{\pp{::}}
% 
\BEP
\textit{identifier}::\textit{statement};
\EEP
% 
Labels are used for resolving time races as follows. Consider a fragment of code such as\index{precedence!using labels}
%
\codefragment{labelled-emits-1}
\BEP
...
\EEP
\codefragment{labelled-emits-2}
%
The labels in the precedence statement quite neatly express that
\pp{emit x(1);} should be executed before \pp{emit x(2);}. Hence
the value of \pp{x} will be $2$ after executing the code above.

Labeling supports a much finer precedence control for resolving time 
races than using method names only. In the (semantically irrelevant) 
piece of code, for instance,
%
\codefragment{labelled-absurd-1}
\BEP
...
\EEP
\codefragment{labelled-absurd-2}
%
a value is reset, then decremented, and reset again.
}

\section{Elementary Examples: Counters}\label{counter}

We discuss a number of elementary examples programming counters. We 
point out possible design decision and highlight some traps one may 
encounter.

\paragraph{Counting using control only.}

Designing counters using control only is a rather restricted 
exercise, 
but quite useful to demonstrate the power of parallel composition.

The most elementary counter probably is a two bit counter that emits
the signal \pp{elapsed} if the signal \pp{incr} has been present
twice.
%
\codeinput{two-bit-counter}\
%
If started the process waits for the presence of the signal 
\pp{incr}. 
In case that \pp{incr} is present, the await process stops, and the 
second await process is started only in the next instant. Again 
presence of \pp{incr} terminates the the await process. Hence in the 
same instant \pp{elapsed} is emitted. 

One should note that the rather similar process
% 
\codeinput{wrong-two-bit-counter}
% 
fails to implement a two bit counter; the second await process is
started at the same instant the first terminates, but terminates
itself instantaneously since \pp{incr} is present.  Hence \pp{elapsed}
is emitted at the first instant the signal \pp{incr} is present.

One may iterate the counting by adding a loop as in
%
\codeinput{looping-two-bit-counter}
%
The signal \pp{elapsed} is always emitted if the signal \pp{incr} is 
present for the second time. We have added the signal \pp{start} to 
start counting only when it is present.\footnote{What happens if the 
second {\tt next} is omitted?}

There is an alternative using the \pp{await next} operator
% 
\codeinput{second-looping-two-bit-counter}
% 
Note, however, that this counter does not start to count in the first 
instant.


Next step is to generate a four bit counter by parallel composition 
of 
two two bit counters.
% 
\codeinput{simple-four-bit-counter}
%
The first two bit counter emits the signal \pp{carry} if the input
signal \pp{incr} has been present twice, the second emits the signal
\pp{elapsed} if \pp{carry} has been present twice.  \pp{carry} is a
local signal. Iteration generates $2^n$ bit counters. 

We may add a tiny bit of sophistication by stipulating that the 
counter should be (re)started only if the signal \pp{start} is 
present.
%
\codeinput{four-bit-counter}
%
Since the parallel process never terminates we have to use preemption 
to restart. 

One may wonder about the last \pp{next} statement in the outer loop. 
One may omit it and find that the compiler issues an error message 
``\pp{instantaneous loop \ldots}''. Of course, analysis of the program 
proves that the signal \pp{elapsed} is never emitted when entering 
the 
outer loop (provided that it is not emitted elsewhere). Hence the 
loop 
is not instantaneous. However, the effort to analyse 
such a fragment for absence of instantaneous loops is 
considerable.\footnote{at worst as difficult as proving 
satisfiability 
of a set of Boolean equations.} Hence we use an algorithm of low 
complexity with the drawback that some ostensibly correct programs 
are 
rejected by the compiler.


\paragraph{Mixing control and data.}

A counter implemented using pure control has the obvious drawback 
that, 
for a counter of length $n$, a different program has to be designed 
for each $n$. A much more flexible solution is obtain if we use some 
integer attribute that is set to $0$ when starting counting, and that 
is incremented whenever the signal \pp{incr} is present.
% 
\codeinput{counter}
% 
\paragraph{Using data methods with almost no control.} There is a 
sort of contrived version of a counter where control is reduced to a 
minimum.
% 
\codeinput{counter-with-data-only}
%

\paragraph{NOTES ON GOOD PRACTICE.}
\begin{itemize}
\item  Using data in reactive applications always
 is a cause of concern in embedded systems since resources are often
 constrained. For instance, the use of values of type float may be
 disastrous on a 8 bit micro controller because representation of floats
 and operations on floats consume a lot of memory and of computation time.
 This holds even if only one variable of type \pp{float} is declared
 since respective software libraries must be loaded if no hardware support
 for floating point operations is provided.
 
 \item A similar argument applies for using fields and methods. Both should be
 declared \pp{static} whenever feasible: then the additional overhead
 for implementing the indirection of calling methods relative to an
 object is avoided. In particular, all fields and methods in a 
 configuration class should be static since the configuration object is
 a singleton, hence no indirection is needed. 
 
 \item Finally, one should always keep in mind the statement
  \begin{quotation}
     \emph{Control is cheap, data are expensive}
  \end{quotation}
  meaning that whenever one can model control using pure signals and 
  the control structures such as loop, preemption, etc. the resulting
  code is extremely efficient, in particular for a micro controller.

\end{itemize}

%\newpage
%\newpage
\section{Reactive Methods}\label{methods}

\paragraph{Calling reactive methods.} So far, we have avoided using
methods for implementing behaviour.  Now being on established grounds,
we consider reactive methods as another means for reusing reactive 
code.

By definition, a method is \emph{reactive}\index{method!reactive}
\index{reactive method|textbf}
 if a reactive statement
occurs in its body, or if the modifier \pp{reactive}\index{reactive@\texttt{reactive}} is applied.  By
default, all other methods are referred to as \emph{data methods}\index{data method}. 
Reactive methods denote processes that may have parameters.

For instance, the now familiar four bit counters of the example above
may be embodied in a reactive method:

\codeinput{two-times-two-bit-counter}

Reactive methods can only be called within the body of an \pp{active}\index{active@\pp{active}} 
statement or from reactive methods. 
In that data methods are ``downward closed'':  the
obvious argument is that the reactive code should control the data
operation, and not vice versa.

The semantics of reactive method calls is defined by syntax 
expansion: the call is replaced by the code obtained from its body by 
replacing the formal by the actual parameters.  

\paragraph{\textit{Labelling once again.}}\index{label!method}

{\em
Method calls can be labelled. The advent of reactive methods 
considerably enhances the power of labels. Consider the following 
fragment of code:

\codefragment{labelled-methods-1}

The label chain of \emph{\pp{l1::l2}} exactly addresses the position of
the reset statement as seen from level of the object as does the
chain \emph{\pp{l1:l3}} for the decrement statement. The labelling 
extends to comparing calls of data methods made in different methods 
as in
 
\codefragment{labelled-methods-2}

One should note that precedence statements can only be specified at
the level of an object.
}

%\newpage
\section{Interfacing to the Environment}\label{interface}

\paragraph{Interfaces for interfacing.} Signals are always private, 
as all fields and methods of an reactive object are. However, sensors or signals may interface to the environment. We distinguish ``input'' sensors\index{sensor!input}\index{input sensor}, ``output'' signals\index{signal!output}\index{output signal}, and ``local'' signals\index{signal!local}\index{local signal}. The kind of signal is determined 
by signal constructor. If the constructor has no argument the signal 
is local. If the signal constructor has one parameter of interface 
type \pp{Output} the signal is an output signal. Constructors of sensors
must always have an argument of interface type \pp{Input}, e.g. 
%
\codefragment{in-out-signals}
% 

The interface types \pp{Input}\index{Input@\pp{Input}}
 and \pp{Output}\index{Output@\pp{Output}}
 are built-in. The interfaces
are so-called marker interfaces meaning that they act as a place holder
lacking any semantic content. Implementations should provide the following
callback methods\index{method!callback}
 depending on the nature of the input or output signal.
\begin{description}
\item[pure input sensors] A parameterless method \pp{new\_val}\index{new\_val@\pp{new\_val}} with result type \texttt{boolean}. 

\item[valued input sensors] A parameterless method \pp{new\_val} with result type \texttt{boolean} and a parameterless method \pp{get\_val}\index{get\_val@\pp{get\_val}} with result type $T$ if the value type of the signal is $T$.

\item[pure output signals] A parameterless void method \pp{put\_val}\index{put\_val@\pp{put\_val}}. 

\item[valued output signals] A void method \pp{put\_val} with a parameter of type $T$ if the value type of the signal is $T$.
\end{description}

According to the synchronous execution model, input sensors are set 
at the beginning of an instant, while output signals are communicated 
to the environment at the end of an instant. Input sensors are set 
using the methods \pp{new\_val} and \pp{get\_val}. If 
\pp{new\_val} returns the value \pp{true}, the sensor is set to be present, 
and its value is updated by the value obtained as a result of 
\pp{get\_val}. Output signals are communicated using the method 
\pp{put\_val}: if an output signal is present the method 
\pp{put\_val} will be called with the actual value of the signal, in case that the signal is valued.

In the example given, the interfaces may be implemented by the types 
%
\codefragment{my-in-out-signals}
% 
Here the methods are assumed to be \emph{native}\index{native}
\index{method!native}, that is implemented in C as 
platform-dependent code. Alternatively, we may have used anonymous classes:
%
\codefragment{anon-in-out-signals}
% 
The marker interfaces are extended by the respective methods.

We summarise: the kind of a signal is determined by its constructor
\begin{itemize}
  \item The type \pp{Sensor<$T$>} has one constructor with a
  parameter of type \pp{Input}. 
  
  \item The type \pp{Signal<$T$>} has two constructors,
     \begin{itemize}
        \item one being parameterless (for ``local'' signals)
        \index{signal!local}, and
        
        \item the other with a parameter of type \pp{Output}.
     \end{itemize} 
\end{itemize}
Implementations of \pp{Input} and \pp{Output} are required to have 
(callback) methods \pp{new\_val} and \pp{get\_val}, resp. 
\pp{put\_val} the type of which must match with the type of the 
signal value. Mismatches raise an error message. We discuss 
the interfaces \pp{Input} and \pp{Output}, and their implementations 
at length in Chapter~\ref{InputOutput}.


\section{\textit{The Type} \texttt{time}}\label{timestamp}

{\em
\paragraph{\textit{Sampling system time.}} 
The synchrony hypothesis captures an abstract notion of time.  As we 
have argued, the abstraction simplifies the task of the designer and 
programmer.  However, one would like to refer to the passing of 
concrete time as well.  Phrases such as ``after x milliseconds do 
\ldots'' come to mind immediately.  Such a concrete notion of time can coexist with abstract time if concrete time is considered as just another input that is sampled at the beginning of each instant. 

In fact, one is rather interested in the passing of time than in time
in absolute terms, i.e. in statements of the form
\begin{itemize}
  \item for how much time a signal has not been present, or
  \item how much time was needed for computing the last instant, or
  \item wait for a specified amount of time
\end{itemize}
rather than a statement
\begin{itemize}
  \item stop the process in the year 2525 exactly at 12 pm, December, 31$^{st}$. 
\end{itemize}
To support concrete time \se\ has a primitive built-in type\index{time@\pp{time}}
% 
$$\mathtt{time}$$
% 
Constants of type \pp{time} are of the form
%
\BEP
                  1hour, 3sec, 100msec, 20 usec, \ldots
\EEP
% 
Statements related to type \pp{time} are of the form
\begin{itemize}
  \item   Waiting for some time\index{await@\pp{await}!timed}, e.g.
	%
    \begin{quote}
	\codefragment{await-time}
    \end{quote}
    	%
	The semantics is natural: it is recorded
	 how much (real) time has passed since the waiting has started.  If the
	 difference is greater than $300$ milliseconds the
     wait process is preempted. 
     
     Note however that the resolution of time depends on the granularity
     of instants: if, for instance, the field \pp{timing} (c.f. below) is set
     to 40ms the await statement will be preempted only after 320 msec since
     7 instants will take only 280 msec, and 8 instants 320 msec. Concrete
     time is measured only at the begining of an instant.

	\item More generally, preemption operators may be enhanced in that the
	conditions $C_{i}$ of
	%
	\BEP
	    cancel [strongly] 
	       $P$
	    when ($C_{1}$) \{ $P_{1}$ \}
	    [ else when ($C_{2}$) \{ $P_{2}$ \}
	      \ldots
	      else when ($C_{2}$) \{ $P_{n}$ \}
	    ];      
	\EEP
	%
	are required to be either Boolean expressions or expressions of
	type \pp{time}.

  \item Timeouts often depend on the concrete time that passed since 
   some sensor or signal $s$ was present for the last time.
   For that purpose, all sensors and signals
   are provided with an operator\index{?@@\pp{?@}}
       $$\texttt{@}s$$
    that returns the time for which the sensor or signal $s$ has not been    
    present. We refer to \pp{@} as the \emph{(a)wait-for-signal} operator.
    
    A typical fragment of code is 
    %
    \begin{quote}
     \codefragment{suspend-for-some-time}
     \end{quote}
   % 
   This ``watchdog'' loop checks the signal \pp{actuator} has not been
   updated for more than $5$ seconds.  In that case an alarm \pp{frozen} is       
   issued.
%   
%    Note that \pp{@$s$} equals $0$ if
%    the sensor or signal $s$ is present at every instant. 

\end{itemize}

\paragraph{\textit{Resolution of time.}} The finest resolution of time is
naturally determined by the system clock. A work station may measure time in
terms of micro- or even nanoseconds, while the time resolution of 
smaller micro controller will rarely exceed milliseconds. 

For \se\ however, the finest resolution of time is determined by the length of time need to compute an instant. This is necessary to conform with the synchrony hypothesis (and seems to be the only way how to reconcile ``abstract'' and ``real'' time in a synchronous setting). Hence we measure ``passing of real time'' in that we measure the delta of time between two instants. The faster the execution of instants proceeds the finer the resolution of concrete time is.

\se\ has two modes to control resolution.
\begin{itemize}
\item The distinguished field \pp{timing}\footnote{For each configuration class \pp{timing} is a predefined static final field of type \pp{time}. It is initialised with the constant \pp{0sec}.} is set to a timing constant not equal to \pp{0}. Then \se\ guarantees that computation of an instant takes (ore or less)\footnote{There is always the problem that computations are needed to determine the amount of time passed. One needs to count the number of machines cycles, which is compiler dependant, and one further relies on the precision of the system clock.} exactly the specified amount of (concrete) time. In case that computation of an instant takes more time the synchrony hypothesis is violated, and an exception is raised.

\item If the field \pp{timing} is set to 0 sec (or equivalent) or not defined at all instants are computed continuously, i.e. computation of an instant starts immediately when the previous instant has terminated. In that case the 
field \pp{timing} has a value being the execution time of the previous instant.
\end{itemize}
There is an alternative way to access the execution time of the previous instant: a distinguished built-in sensor (signal) \pp{dt} (``delta t'') has a value of type \pp{double} yields the execution time in terms of fractions of a second. This is useful when approximating differential equations by difference equations (cf. Chapter \ref{data-flow}).
}




